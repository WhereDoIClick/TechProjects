{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab27932e",
   "metadata": {},
   "source": [
    "# Exercise - Get the rock sample data into Visual Studio Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64f8cf-f708-4b06-8945-5176bd7d00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fbabe2-632e-4b9e-81cc-41c07802875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"TestSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.sql.warehouse.dir\", \"/hive/warehouse/dir\") \\\n",
    "#    .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a95b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data file\n",
    "\n",
    "df = spark.read.options(inferSchema='True', header= 'True').csv(\"file:///C:/Users/manso/LocalDocuments/10-TechProjects/over-the-moon/sample-return/data/rocksamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c574285b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Schema:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[ID: int, Mission: string, Type: string, Subtype: string, Weight (g): double, Pristine (%): double]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(\"Schema:\")\n",
    "\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd75af68-3869-429c-80e8-b759ec21b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+--------+----------+------------+\n",
      "|   ID| Mission|   Type| Subtype|Weight (g)|Pristine (%)|\n",
      "+-----+--------+-------+--------+----------+------------+\n",
      "|10001|Apollo11|   Soil|Unsieved|     125.8|       88.36|\n",
      "|10002|Apollo11|   Soil|Unsieved|    5629.0|       93.73|\n",
      "|10003|Apollo11| Basalt|Ilmenite|     213.0|       65.56|\n",
      "|10004|Apollo11|   Core|Unsieved|      44.8|       71.76|\n",
      "|10005|Apollo11|   Core|Unsieved|      53.4|       40.31|\n",
      "|10008|Apollo11|   Soil|Unsieved|      89.0|        5.75|\n",
      "|10009|Apollo11|Breccia|Regolith|     112.0|       97.27|\n",
      "|10010|Apollo11|   Soil|Unsieved|     491.0|       91.03|\n",
      "|10011|Apollo11|   Soil|Unsieved|      82.6|       62.01|\n",
      "|10014|Apollo11|   Soil|Unsieved|      50.0|         0.0|\n",
      "+-----+--------+-------+--------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65667ea9",
   "metadata": {},
   "source": [
    "# Exercise - Determine the question to ask to inform data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e269f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,lag, lit, round, mean as _mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9929b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10001, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=125.8, Pristine (%)=88.36, Weight(kg)=0.1258),\n",
       " Row(ID=10002, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=5629.0, Pristine (%)=93.73, Weight(kg)=5.6290000000000004),\n",
       " Row(ID=10003, Mission='Apollo11', Type='Basalt', Subtype='Ilmenite', Weight(g)=213.0, Pristine (%)=65.56, Weight(kg)=0.213),\n",
       " Row(ID=10004, Mission='Apollo11', Type='Core', Subtype='Unsieved', Weight(g)=44.8, Pristine (%)=71.76, Weight(kg)=0.0448),\n",
       " Row(ID=10005, Mission='Apollo11', Type='Core', Subtype='Unsieved', Weight(g)=53.4, Pristine (%)=40.31, Weight(kg)=0.0534),\n",
       " Row(ID=10008, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=89.0, Pristine (%)=5.75, Weight(kg)=0.089),\n",
       " Row(ID=10009, Mission='Apollo11', Type='Breccia', Subtype='Regolith', Weight(g)=112.0, Pristine (%)=97.27, Weight(kg)=0.112),\n",
       " Row(ID=10010, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=491.0, Pristine (%)=91.03, Weight(kg)=0.491),\n",
       " Row(ID=10011, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=82.6, Pristine (%)=62.01, Weight(kg)=0.08259999999999999),\n",
       " Row(ID=10014, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=50.0, Pristine (%)=0.0, Weight(kg)=0.05)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the sample weight\n",
    "\n",
    "\n",
    "# Rename Column to remove space\n",
    "df = df.withColumnRenamed(\"Weight (g)\",\"Weight(g)\")\n",
    "\n",
    "df = df.withColumn(\"Weight(kg)\", col(\"Weight(g)\") * 0.001)\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa566d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Mission='Apollo15'),\n",
       " Row(Mission='Apollo11'),\n",
       " Row(Mission='Apollo14'),\n",
       " Row(Mission='Apollo12'),\n",
       " Row(Mission='Apollo17'),\n",
       " Row(Mission='Apollo16')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame called missions that will be a summary of data for each of the six Apollo missions that brought samples back. \n",
    "# Create a column in this DataFrame called Mission that has one row for each mission.\n",
    "\n",
    "missions = df.dropDuplicates([\"Mission\"]).select(\"Mission\")\n",
    "missions.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1e5a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missions.toPandas().info()\n",
    "\n",
    "type(missions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5156038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "| Mission| Sample_weight(kg)|\n",
      "+--------+------------------+\n",
      "|Apollo11|          21.55424|\n",
      "|Apollo12|          34.34238|\n",
      "|Apollo14|          41.83363|\n",
      "|Apollo15| 75.39910000000005|\n",
      "|Apollo16| 92.46262000000006|\n",
      "|Apollo17|109.44402000000001|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum total sample weight by mission\n",
    "\n",
    "sample_total_weight = df.groupby('Mission').sum('Weight(kg)')\n",
    "\n",
    "\n",
    "# Using Join expression and remove duplicate columns\n",
    "missions = missions.join(sample_total_weight,missions[\"Mission\"] == sample_total_weight[\"Mission\"]) \\\n",
    "    .select(missions[\"Mission\"], sample_total_weight[\"sum(Weight(kg))\"]) \\\n",
    "    .orderBy(missions[\"Mission\"])\n",
    "\n",
    "# Rename Column\n",
    "missions = missions.withColumnRenamed(\"sum(Weight(kg))\",\"Sample_weight(kg)\")\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d01ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference in weights across missions\n",
    "\n",
    "# Create window\n",
    "windowSpec  = Window.orderBy(\"Mission\")\n",
    "\n",
    "#Simulate Pandas diff() API on PySpark usinl lag function (with above windowSpec)\n",
    "missions = missions.withColumn(\"lag\",lag(\"Sample_weight(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"Weight_diff\", col(\"Sample_weight(kg)\") - col(\"lag\")) \\\n",
    "      .select(\"Mission\",\"Sample_weight(kg)\",\"Weight_diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6ae6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "| Mission| Sample_weight(kg)|       Weight_diff|\n",
      "+--------+------------------+------------------+\n",
      "|Apollo11|          21.55424|               0.0|\n",
      "|Apollo12|          34.34238|12.788139999999999|\n",
      "|Apollo14|          41.83363| 7.491250000000001|\n",
      "|Apollo15| 75.39910000000005| 33.56547000000005|\n",
      "|Apollo16| 92.46262000000006| 17.06352000000001|\n",
      "|Apollo17|109.44402000000001| 16.98139999999995|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace Null values\n",
    "\n",
    "missions = missions.na.fill(value=0,subset=[\"Weight_diff\"])\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510c54eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o99.saveAsTable.\n: java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n\tat org.sparkproject.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1659)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:658)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE DATABASE IF NOT EXISTS train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Write to (managed) tables on train database\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.rockSamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m missions\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.Missions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o99.saveAsTable.\n: java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n\tat org.sparkproject.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1659)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:658)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n"
     ]
    }
   ],
   "source": [
    "# Create train database \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS train\")\n",
    "\n",
    "# Write to (managed) tables on train database\n",
    "\n",
    "df.write.mode(\"Overwrite\").saveAsTable(\"train.rockSamples\")\n",
    "missions.write.mode(\"Overwrite\").saveAsTable(\"train.Missions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99699838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|    train|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='missions', catalog='spark_catalog', namespace=['train'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='rocksamples', catalog='spark_catalog', namespace=['train'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='a', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='AuxCM', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='AuxLM', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='AuxMissions', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List existing databases\n",
    "spark.sql('show databases').show()\n",
    "\n",
    "# List train database tables\n",
    "spark.catalog.listTables('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb83345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+\n",
      "| Mission|Sample_weight(kg)|      Weight_diff|\n",
      "+--------+-----------------+-----------------+\n",
      "|Apollo14|         41.83363|7.491250000000001|\n",
      "+--------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test previous saved tables\n",
    "\n",
    "spark.sql(\"SELECT * FROM train.Missions WHERE Mission='Apollo14'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de758dff",
   "metadata": {},
   "source": [
    "# Exercise - Add rocket weight data to the mission analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "146a8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Total_weight_diff: long (nullable = true)\n",
      " |-- Crewed_area_Payload: double (nullable = true)\n",
      " |-- Sample_Crewed_area: double (nullable = true)\n",
      " |-- Sample_Payload: double (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      "\n",
      "+-----------------+-----------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+--------+------------------+-----------+--------------------+-----------+\n",
      "|Sample_weight(kg)|Weight_diff|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|Total_weight(kg)|Total_weight_diff|Crewed_area_Payload|Sample_Crewed_area|Sample_Payload| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|\n",
      "+-----------------+-----------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+--------+------------------+-----------+--------------------+-----------+\n",
      "|          75.3991|   33.56547|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|           22305|             1283|           0.512759|           0.00338|      0.001733|Apollo15|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|\n",
      "|         21.55424|        0.0|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|           20663|                0|           0.475011|          0.001043|       4.95E-4|Apollo11|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|\n",
      "|         41.83363|    7.49125|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|           21022|              178|           0.483264|           0.00199|       9.62E-4|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|\n",
      "|         34.34238|   12.78814|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|           20844|              181|           0.479172|          0.001648|       7.89E-4|Apollo12|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|\n",
      "|        109.44402|    16.9814|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|           22416|              131|            0.51531|          0.004882|      0.002516|Apollo17|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|\n",
      "|         92.46262|   17.06352|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|           22285|              -20|           0.512299|          0.004149|      0.002126|Apollo16|Challenger (LM-12)|      16456|    America (CM-114)|       5960|\n",
      "+-----------------+-----------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+--------+------------------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add in command and lunar module data\n",
    "\n",
    "lunarModuleData = [(\"Eagle (LM-5)\",15103), \n",
    "        (\"Intrepid (LM-6)\",15235), \n",
    "        (\"Antares (LM-8)\",15264), \n",
    "        (\"Falcon (LM-10)\",16430),\n",
    "        (\"Orion (LM-11)\",16445),\n",
    "        (\"Challenger (LM-12)\",16456)\n",
    "      ]\n",
    "\n",
    "commandModuleData = [(\"Columbia (CSM-107)\",5560), \n",
    "        (\"Yankee Clipper (CM-108)\",5609), \n",
    "        (\"Kitty Hawk (CM-110)\",5758), \n",
    "        (\"Endeavor (CM-112)\",5875),\n",
    "        (\"Casper (CM-113)\",5840),\n",
    "        (\"America (CM-114)\",5960)\n",
    "      ]\n",
    "\n",
    "\n",
    "# Generate ID Column on Dataframes to be able to join them after\n",
    "dfLM = spark.createDataFrame(lunarModuleData,[\"Lunar_module(LM)\", \"LM_mass(kg)\"])\n",
    "dfLM.createOrReplaceTempView('AuxLM')\n",
    "dfLM = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxLM')\n",
    "\n",
    "#dfLM.show()\n",
    "\n",
    "dfCM = spark.createDataFrame(commandModuleData, [\"Command_module(CM)\", \"CM_mass(kg)\"])\n",
    "dfCM.createOrReplaceTempView('AuxCM')\n",
    "dfCM = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxCM')\n",
    "\n",
    "#dfCM.show()\n",
    "\n",
    "dfMissions = missions.select(\"Mission\")\n",
    "dfMissions.createOrReplaceTempView('AuxMissions')\n",
    "dfMissions = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxMissions')\n",
    "\n",
    "#dfMissions.show()\n",
    "\n",
    "# Join Dataframes using ID column\n",
    "dfAll = dfMissions.join(dfLM,dfMissions[\"Row_Num\"] == dfLM[\"Row_Num\"]) \\\n",
    "        .drop(dfMissions[\"Row_Num\"])\n",
    "      \n",
    "dfAll = dfAll.join(dfCM,dfAll[\"Row_Num\"] == dfCM[\"Row_Num\"]) \\\n",
    "        .drop(dfAll[\"Row_Num\"], dfCM[\"Row_Num\"])\n",
    "\n",
    "\n",
    "#dfAll.printSchema()\n",
    "#dfAll.show()\n",
    "\n",
    "# Update missions Dataframe with the new fields\n",
    "missions = missions.join(dfAll,missions[\"Mission\"] == dfAll[\"Mission\"]) \\\n",
    "        .drop(dfAll[\"Mission\"])\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f94f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = false)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "| Sample_weight(kg)|       Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "|          21.55424|               0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|\n",
      "|          34.34238|12.788139999999999|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|\n",
      "|          41.83363| 7.491250000000001|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|\n",
      "| 75.39910000000005| 33.56547000000005|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|\n",
      "| 92.46262000000006| 17.06352000000001|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|\n",
      "|109.44402000000001| 16.98139999999995|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the difference in weights across missions\n",
    "\n",
    "# Create window\n",
    "windowSpec  = Window.orderBy(\"Mission\")\n",
    "\n",
    "#Simulate Pandas diff() API on PySpark usinl lag function (with above windowSpec)\n",
    "missions = missions.withColumn(\"lag\",lag(\"LM_mass(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"LM_mass_diff\", col(\"LM_mass(kg)\") - col(\"lag\")) \\\n",
    "      .drop(\"lag\")\n",
    "\n",
    "\n",
    "missions = missions.withColumn(\"lag\",lag(\"CM_mass(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"CM_mass_diff\", col(\"CM_mass(kg)\") - col(\"lag\")) \\\n",
    "      .drop(\"lag\")\n",
    "\n",
    "# Replace Null values\n",
    "missions = missions.na.fill(value=0,subset=[\"LM_mass_diff\"])\n",
    "missions = missions.na.fill(value=0,subset=[\"CM_mass_diff\"])\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e62c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = false)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Total_weight_diff: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "| Sample_weight(kg)|       Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|Total_weight(kg)|Total_weight_diff|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "|          21.55424|               0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|           20663|                0|\n",
      "|          34.34238|12.788139999999999|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|           20844|              181|\n",
      "|          41.83363| 7.491250000000001|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|           21022|              178|\n",
      "| 75.39910000000005| 33.56547000000005|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|           22305|             1283|\n",
      "| 92.46262000000006| 17.06352000000001|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|           22285|              -20|\n",
      "|109.44402000000001| 16.98139999999995|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|           22416|              131|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add some totals for each mission across both the lunar and command modules\n",
    "\n",
    "missions = missions.withColumn(\"Total_weight(kg)\", col(\"LM_mass(kg)\") + col(\"CM_mass(kg)\")) \\\n",
    "      .withColumn(\"Total_weight_diff\", col(\"LM_mass_diff\") + col(\"CM_mass_diff\")) \\\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09317f",
   "metadata": {},
   "source": [
    "# Exercise - Understand the data in the missions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3393b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Total_weight_diff: long (nullable = true)\n",
      " |-- Crewed_area_Payload: double (nullable = true)\n",
      " |-- Sample_Crewed_area: double (nullable = true)\n",
      " |-- Sample_Payload: double (nullable = true)\n",
      "\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "|Sample_weight(kg)|Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|Total_weight(kg)|Total_weight_diff|Crewed_area_Payload|Sample_Crewed_area|Sample_Payload|\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "|         21.55424|        0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|           20663|                0|           0.475011|          0.001043|       4.95E-4|\n",
      "|         34.34238|   12.78814|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|           20844|              181|           0.479172|          0.001648|       7.89E-4|\n",
      "|         41.83363|    7.49125|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|           21022|              178|           0.483264|           0.00199|       9.62E-4|\n",
      "|          75.3991|   33.56547|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|           22305|             1283|           0.512759|           0.00338|      0.001733|\n",
      "|         92.46262|   17.06352|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|           22285|              -20|           0.512299|          0.004149|      0.002126|\n",
      "|        109.44402|    16.9814|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|           22416|              131|            0.51531|          0.004882|      0.002516|\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample-to-weight ratio\n",
    "\n",
    "saturnVPayload = 43500\n",
    "\n",
    "missions = missions.withColumn(\"Crewed_area_Payload\", col(\"Total_weight(kg)\") / saturnVPayload) \\\n",
    "    .withColumn(\"Sample_Crewed_area\", col(\"Sample_weight(kg)\") / col(\"Total_weight(kg)\")) \\\n",
    "    .withColumn(\"Sample_Payload\", round(col(\"Sample_weight(kg)\") / saturnVPayload,6))\n",
    "\n",
    "# Round all numeric columns to 6 decimal places\n",
    "for c_name, c_type in missions.dtypes:\n",
    "    if c_type in ('long','double', 'float'):\n",
    "        missions = missions.withColumn(c_name, round(c_name, 6))\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c66620d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o247.saveAsTable.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Update (managed) Missions table on train database\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m missions\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.Missions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect * From train.Missions;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDESCRIBE TABLE train.Missions;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o247.saveAsTable.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n"
     ]
    }
   ],
   "source": [
    "# Update (managed) Missions table on train database\n",
    "\n",
    "missions.write.mode(\"Overwrite\").saveAsTable(\"train.Missions\")\n",
    "\n",
    "spark.sql(\"Select * From train.Missions;\").show()\n",
    "spark.sql(\"DESCRIBE TABLE train.Missions;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1f1c07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the average of all those ratios across all the missions\n",
    "\n",
    "# Note: (...).collect()[0][0] was used to allow a single value in variable instead of dataframe\n",
    "crewedArea_payload_ratio = missions.select(round(_mean(\"Crewed_area_Payload\"),6)).collect()[0][0]\n",
    "sample_crewedArea_ratio = missions.select(round(_mean(\"Sample_Crewed_area\"),6)).collect()[0][0]\n",
    "sample_payload_ratio = missions.select(round(_mean(col(\"Sample_Payload\")),6)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab2bc3",
   "metadata": {},
   "source": [
    "# Exercise - Predict Artemis sample capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6cb5cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('artemis1', 26520, 26988) ('artemis1b', 26520, 37965) ('artemis2', 26520, 42955)\n",
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|\n",
      "+---------+----------------+-----------+\n",
      "| artemis1|           26520|      26988|\n",
      "|artemis1b|           26520|      37965|\n",
      "| artemis2|           26520|      42955|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Artemis mission DataFrame\n",
    "artemis_crewedArea = 26520\n",
    "\n",
    "missionData = [\"artemis1\",\"artemis1b\",\"artemis2\"]\n",
    "weightData = [artemis_crewedArea,artemis_crewedArea,artemis_crewedArea]\n",
    "payloadData = [26988, 37965, 42955]\n",
    "\n",
    "# joins / Zip the 3 configuration lists\n",
    "artemisAllData = zip(missionData,weightData,payloadData)\n",
    "\n",
    "# Convert zip object to a list\n",
    "artemisAllData = list(artemisAllData)\n",
    "\n",
    "#print(artemisAllData)\n",
    "print(*artemisAllData)\n",
    "\n",
    "# Schema configuration\n",
    "schemaArtemis = [\"Mission\", \"Total_weight(kg)\", \"Payload(kg)\"]\n",
    "\n",
    "artemis_mission = spark.createDataFrame(data = artemisAllData, schema = schemaArtemis)\n",
    "\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6ec5cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      " |-- Sample_weight_from_total(kg): double (nullable = true)\n",
      " |-- Sample_weight_from_payload(kg): double (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "| artemis1|           26520|      26988|                    75.55548|                     38.781756|\n",
      "|artemis1b|           26520|      37965|                    75.55548|                     54.555705|\n",
      "| artemis2|           26520|      42955|                    75.55548|                     61.726335|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate the weight of samples based on the ratios we determined from the Apollo missions\n",
    "\n",
    "artemis_mission = artemis_mission.withColumn(\"Sample_weight_from_total(kg)\",round(col(\"Total_weight(kg)\") * sample_crewedArea_ratio,6))\n",
    "artemis_mission = artemis_mission.withColumn(\"Sample_weight_from_payload(kg)\",round(col(\"Payload(kg)\") * sample_payload_ratio,6))\n",
    "\n",
    "\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fe41cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "|summary| Mission|Total_weight(kg)|       Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|Estimated_sample_weight(kg)|\n",
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "|  count|       3|               3|                 3|                           3|                             3|                          3|\n",
      "|   mean|    NULL|         26520.0|35969.333333333336|                    75.55548|            51.687931999999996|          63.62170633333333|\n",
      "| stddev|    NULL|             0.0| 8168.432305732437|                         0.0|             11.73803722333751|          5.869018886548102|\n",
      "|    min|artemis1|           26520|             26988|                    75.55548|                     38.781756|                  57.168618|\n",
      "|    max|artemis2|           26520|             42955|                    75.55548|                     61.726335|                  68.640908|\n",
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "\n",
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      " |-- Sample_weight_from_total(kg): double (nullable = true)\n",
      " |-- Sample_weight_from_payload(kg): double (nullable = true)\n",
      " |-- Estimated_sample_weight(kg): double (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|Estimated_sample_weight(kg)|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "| artemis1|           26520|      26988|                    75.55548|                     38.781756|                  57.168618|\n",
      "|artemis1b|           26520|      37965|                    75.55548|                     54.555705|                  65.055593|\n",
      "| artemis2|           26520|      42955|                    75.55548|                     61.726335|                  68.640908|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the average of the two predictions:\n",
    "artemis_mission = artemis_mission.withColumn(\"Estimated_sample_weight(kg)\", round((col(\"Sample_weight_from_payload(kg)\") + col(\"Sample_weight_from_total(kg)\"))/2,6))\n",
    "\n",
    "artemis_mission.describe().show()\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7730b",
   "metadata": {},
   "source": [
    "# Exercise - Prioritize Moon rock sample gathering based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fbec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
