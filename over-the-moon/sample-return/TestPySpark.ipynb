{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab27932e",
   "metadata": {},
   "source": [
    "# Exercise - Get the rock sample data into Visual Studio Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec64f8cf-f708-4b06-8945-5176bd7d00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fbabe2-632e-4b9e-81cc-41c07802875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"TestSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.sql.warehouse.dir\", \"/hive/warehouse/dir\") \\\n",
    "#    .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a95b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data file\n",
    "\n",
    "df = spark.read.options(inferSchema='True', header= 'True').csv(\"file:///C:/Users/manso/LocalDocuments/10-TechProjects/over-the-moon/sample-return/data/rocksamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c574285b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Schema:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[ID: int, Mission: string, Type: string, Subtype: string, Weight (g): double, Pristine (%): double]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(\"Schema:\")\n",
    "\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd75af68-3869-429c-80e8-b759ec21b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+--------+----------+------------+\n",
      "|   ID| Mission|   Type| Subtype|Weight (g)|Pristine (%)|\n",
      "+-----+--------+-------+--------+----------+------------+\n",
      "|10001|Apollo11|   Soil|Unsieved|     125.8|       88.36|\n",
      "|10002|Apollo11|   Soil|Unsieved|    5629.0|       93.73|\n",
      "|10003|Apollo11| Basalt|Ilmenite|     213.0|       65.56|\n",
      "|10004|Apollo11|   Core|Unsieved|      44.8|       71.76|\n",
      "|10005|Apollo11|   Core|Unsieved|      53.4|       40.31|\n",
      "|10008|Apollo11|   Soil|Unsieved|      89.0|        5.75|\n",
      "|10009|Apollo11|Breccia|Regolith|     112.0|       97.27|\n",
      "|10010|Apollo11|   Soil|Unsieved|     491.0|       91.03|\n",
      "|10011|Apollo11|   Soil|Unsieved|      82.6|       62.01|\n",
      "|10014|Apollo11|   Soil|Unsieved|      50.0|         0.0|\n",
      "+-----+--------+-------+--------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65667ea9",
   "metadata": {},
   "source": [
    "# Exercise - Determine the question to ask to inform data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5e269f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,lag, lit, round, mean as _mean, sum as _sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9929b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10001, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=125.8, Pristine (%)=88.36, Weight(kg)=0.1258),\n",
       " Row(ID=10002, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=5629.0, Pristine (%)=93.73, Weight(kg)=5.6290000000000004),\n",
       " Row(ID=10003, Mission='Apollo11', Type='Basalt', Subtype='Ilmenite', Weight(g)=213.0, Pristine (%)=65.56, Weight(kg)=0.213),\n",
       " Row(ID=10004, Mission='Apollo11', Type='Core', Subtype='Unsieved', Weight(g)=44.8, Pristine (%)=71.76, Weight(kg)=0.0448),\n",
       " Row(ID=10005, Mission='Apollo11', Type='Core', Subtype='Unsieved', Weight(g)=53.4, Pristine (%)=40.31, Weight(kg)=0.0534),\n",
       " Row(ID=10008, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=89.0, Pristine (%)=5.75, Weight(kg)=0.089),\n",
       " Row(ID=10009, Mission='Apollo11', Type='Breccia', Subtype='Regolith', Weight(g)=112.0, Pristine (%)=97.27, Weight(kg)=0.112),\n",
       " Row(ID=10010, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=491.0, Pristine (%)=91.03, Weight(kg)=0.491),\n",
       " Row(ID=10011, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=82.6, Pristine (%)=62.01, Weight(kg)=0.08259999999999999),\n",
       " Row(ID=10014, Mission='Apollo11', Type='Soil', Subtype='Unsieved', Weight(g)=50.0, Pristine (%)=0.0, Weight(kg)=0.05)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the sample weight\n",
    "\n",
    "\n",
    "# Rename Column to remove space\n",
    "df = df.withColumnRenamed(\"Weight (g)\",\"Weight(g)\")\n",
    "\n",
    "df = df.withColumn(\"Weight(kg)\", col(\"Weight(g)\") * 0.001)\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa566d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Mission='Apollo15'),\n",
       " Row(Mission='Apollo11'),\n",
       " Row(Mission='Apollo14'),\n",
       " Row(Mission='Apollo12'),\n",
       " Row(Mission='Apollo17'),\n",
       " Row(Mission='Apollo16')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame called missions that will be a summary of data for each of the six Apollo missions that brought samples back. \n",
    "# Create a column in this DataFrame called Mission that has one row for each mission.\n",
    "\n",
    "missions = df.dropDuplicates([\"Mission\"]).select(\"Mission\")\n",
    "missions.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1e5a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missions.toPandas().info()\n",
    "\n",
    "type(missions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5156038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "| Mission| Sample_weight(kg)|\n",
      "+--------+------------------+\n",
      "|Apollo11|          21.55424|\n",
      "|Apollo12|          34.34238|\n",
      "|Apollo14|          41.83363|\n",
      "|Apollo15| 75.39910000000005|\n",
      "|Apollo16| 92.46262000000006|\n",
      "|Apollo17|109.44402000000001|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum total sample weight by mission\n",
    "\n",
    "sample_total_weight = df.groupby('Mission').sum('Weight(kg)')\n",
    "\n",
    "\n",
    "# Using Join expression and remove duplicate columns\n",
    "missions = missions.join(sample_total_weight,missions[\"Mission\"] == sample_total_weight[\"Mission\"]) \\\n",
    "    .select(missions[\"Mission\"], sample_total_weight[\"sum(Weight(kg))\"]) \\\n",
    "    .orderBy(missions[\"Mission\"])\n",
    "\n",
    "# Rename Column\n",
    "missions = missions.withColumnRenamed(\"sum(Weight(kg))\",\"Sample_weight(kg)\")\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d01ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference in weights across missions\n",
    "\n",
    "# Create window\n",
    "windowSpec  = Window.orderBy(\"Mission\")\n",
    "\n",
    "#Simulate Pandas diff() API on PySpark usinl lag function (with above windowSpec)\n",
    "missions = missions.withColumn(\"lag\",lag(\"Sample_weight(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"Weight_diff\", col(\"Sample_weight(kg)\") - col(\"lag\")) \\\n",
    "      .select(\"Mission\",\"Sample_weight(kg)\",\"Weight_diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6ae6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "| Mission| Sample_weight(kg)|       Weight_diff|\n",
      "+--------+------------------+------------------+\n",
      "|Apollo11|          21.55424|               0.0|\n",
      "|Apollo12|          34.34238|12.788139999999999|\n",
      "|Apollo14|          41.83363| 7.491250000000001|\n",
      "|Apollo15| 75.39910000000005| 33.56547000000005|\n",
      "|Apollo16| 92.46262000000006| 17.06352000000001|\n",
      "|Apollo17|109.44402000000001| 16.98139999999995|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace Null values\n",
    "\n",
    "missions = missions.na.fill(value=0,subset=[\"Weight_diff\"])\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510c54eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o99.saveAsTable.\n: java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n\tat org.sparkproject.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1659)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:658)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE DATABASE IF NOT EXISTS train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Write to (managed) tables on train database\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.rockSamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m missions\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.Missions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o99.saveAsTable.\n: java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\r\n\tat org.sparkproject.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n\tat org.sparkproject.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1659)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:658)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n"
     ]
    }
   ],
   "source": [
    "# Create train database \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS train\")\n",
    "\n",
    "# Write to (managed) tables on train database\n",
    "\n",
    "df.write.mode(\"Overwrite\").saveAsTable(\"train.rockSamples\")\n",
    "missions.write.mode(\"Overwrite\").saveAsTable(\"train.Missions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99699838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|    train|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='rocksamples', catalog='spark_catalog', namespace=['train'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List existing databases\n",
    "spark.sql('show databases').show()\n",
    "\n",
    "# List train database tables\n",
    "spark.catalog.listTables('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb83345",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `train`.`Missions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'Filter ('Mission = Apollo14)\n   +- 'UnresolvedRelation [train, Missions], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test previous saved tables\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM train.Missions WHERE Mission=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApollo14\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `train`.`Missions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'Filter ('Mission = Apollo14)\n   +- 'UnresolvedRelation [train, Missions], [], false\n"
     ]
    }
   ],
   "source": [
    "# Test previous saved tables\n",
    "\n",
    "spark.sql(\"SELECT * FROM train.Missions WHERE Mission='Apollo14'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de758dff",
   "metadata": {},
   "source": [
    "# Exercise - Add rocket weight data to the mission analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146a8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = false)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+\n",
      "| Sample_weight(kg)|       Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+\n",
      "|          21.55424|               0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|\n",
      "|109.44402000000001| 16.98139999999995|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|\n",
      "|          41.83363| 7.491250000000001|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|\n",
      "| 92.46262000000006| 17.06352000000001|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|\n",
      "| 75.39910000000005| 33.56547000000005|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|\n",
      "|          34.34238|12.788139999999999|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add in command and lunar module data\n",
    "\n",
    "lunarModuleData = [(\"Eagle (LM-5)\",15103), \n",
    "        (\"Intrepid (LM-6)\",15235), \n",
    "        (\"Antares (LM-8)\",15264), \n",
    "        (\"Falcon (LM-10)\",16430),\n",
    "        (\"Orion (LM-11)\",16445),\n",
    "        (\"Challenger (LM-12)\",16456)\n",
    "      ]\n",
    "\n",
    "commandModuleData = [(\"Columbia (CSM-107)\",5560), \n",
    "        (\"Yankee Clipper (CM-108)\",5609), \n",
    "        (\"Kitty Hawk (CM-110)\",5758), \n",
    "        (\"Endeavor (CM-112)\",5875),\n",
    "        (\"Casper (CM-113)\",5840),\n",
    "        (\"America (CM-114)\",5960)\n",
    "      ]\n",
    "\n",
    "\n",
    "# Generate ID Column on Dataframes to be able to join them after\n",
    "dfLM = spark.createDataFrame(lunarModuleData,[\"Lunar_module(LM)\", \"LM_mass(kg)\"])\n",
    "dfLM.createOrReplaceTempView('AuxLM')\n",
    "dfLM = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxLM')\n",
    "\n",
    "#dfLM.show()\n",
    "\n",
    "dfCM = spark.createDataFrame(commandModuleData, [\"Command_module(CM)\", \"CM_mass(kg)\"])\n",
    "dfCM.createOrReplaceTempView('AuxCM')\n",
    "dfCM = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxCM')\n",
    "\n",
    "#dfCM.show()\n",
    "\n",
    "dfMissions = missions.select(\"Mission\")\n",
    "dfMissions.createOrReplaceTempView('AuxMissions')\n",
    "dfMissions = spark.sql('select ROW_NUMBER() OVER(ORDER BY (Select 0)) AS Row_Num, * from AuxMissions')\n",
    "\n",
    "#dfMissions.show()\n",
    "\n",
    "# Join Dataframes using ID column\n",
    "dfAll = dfMissions.join(dfLM,dfMissions[\"Row_Num\"] == dfLM[\"Row_Num\"]) \\\n",
    "        .drop(dfMissions[\"Row_Num\"])\n",
    "      \n",
    "dfAll = dfAll.join(dfCM,dfAll[\"Row_Num\"] == dfCM[\"Row_Num\"]) \\\n",
    "        .drop(dfAll[\"Row_Num\"], dfCM[\"Row_Num\"])\n",
    "\n",
    "\n",
    "#dfAll.printSchema()\n",
    "#dfAll.show()\n",
    "\n",
    "# Update missions Dataframe with the new fields\n",
    "missions = missions.join(dfAll,missions[\"Mission\"] == dfAll[\"Mission\"]) \\\n",
    "        .drop(dfAll[\"Mission\"])\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97f94f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = false)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "| Sample_weight(kg)|       Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "|          21.55424|               0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|\n",
      "|          34.34238|12.788139999999999|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|\n",
      "|          41.83363| 7.491250000000001|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|\n",
      "| 75.39910000000005| 33.56547000000005|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|\n",
      "| 92.46262000000006| 17.06352000000001|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|\n",
      "|109.44402000000001| 16.98139999999995|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the difference in weights across missions\n",
    "\n",
    "# Create window\n",
    "windowSpec  = Window.orderBy(\"Mission\")\n",
    "\n",
    "#Simulate Pandas diff() API on PySpark usinl lag function (with above windowSpec)\n",
    "missions = missions.withColumn(\"lag\",lag(\"LM_mass(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"LM_mass_diff\", col(\"LM_mass(kg)\") - col(\"lag\")) \\\n",
    "      .drop(\"lag\")\n",
    "\n",
    "\n",
    "missions = missions.withColumn(\"lag\",lag(\"CM_mass(kg)\",1).over(windowSpec)) \\\n",
    "      .withColumn(\"CM_mass_diff\", col(\"CM_mass(kg)\") - col(\"lag\")) \\\n",
    "      .drop(\"lag\")\n",
    "\n",
    "# Replace Null values\n",
    "missions = missions.na.fill(value=0,subset=[\"LM_mass_diff\"])\n",
    "missions = missions.na.fill(value=0,subset=[\"CM_mass_diff\"])\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e62c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = false)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Total_weight_diff: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "| Sample_weight(kg)|       Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|Total_weight(kg)|Total_weight_diff|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "|          21.55424|               0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|           20663|                0|\n",
      "|          34.34238|12.788139999999999|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|           20844|              181|\n",
      "|          41.83363| 7.491250000000001|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|           21022|              178|\n",
      "| 75.39910000000005| 33.56547000000005|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|           22305|             1283|\n",
      "| 92.46262000000006| 17.06352000000001|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|           22285|              -20|\n",
      "|109.44402000000001| 16.98139999999995|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|           22416|              131|\n",
      "+------------------+------------------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add some totals for each mission across both the lunar and command modules\n",
    "\n",
    "missions = missions.withColumn(\"Total_weight(kg)\", col(\"LM_mass(kg)\") + col(\"CM_mass(kg)\")) \\\n",
    "      .withColumn(\"Total_weight_diff\", col(\"LM_mass_diff\") + col(\"CM_mass_diff\")) \\\n",
    "\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09317f",
   "metadata": {},
   "source": [
    "# Exercise - Understand the data in the missions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3393b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample_weight(kg): double (nullable = true)\n",
      " |-- Weight_diff: double (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Lunar_module(LM): string (nullable = true)\n",
      " |-- LM_mass(kg): long (nullable = true)\n",
      " |-- Command_module(CM): string (nullable = true)\n",
      " |-- CM_mass(kg): long (nullable = true)\n",
      " |-- LM_mass_diff: long (nullable = true)\n",
      " |-- CM_mass_diff: long (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Total_weight_diff: long (nullable = true)\n",
      " |-- Crewed_area_Payload: double (nullable = true)\n",
      " |-- Sample_Crewed_area: double (nullable = true)\n",
      " |-- Sample_Payload: double (nullable = true)\n",
      "\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "|Sample_weight(kg)|Weight_diff| Mission|  Lunar_module(LM)|LM_mass(kg)|  Command_module(CM)|CM_mass(kg)|LM_mass_diff|CM_mass_diff|Total_weight(kg)|Total_weight_diff|Crewed_area_Payload|Sample_Crewed_area|Sample_Payload|\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "|         21.55424|        0.0|Apollo11|      Eagle (LM-5)|      15103|  Columbia (CSM-107)|       5560|           0|           0|           20663|                0|           0.475011|          0.001043|       4.95E-4|\n",
      "|         34.34238|   12.78814|Apollo12|   Intrepid (LM-6)|      15235|Yankee Clipper (C...|       5609|         132|          49|           20844|              181|           0.479172|          0.001648|       7.89E-4|\n",
      "|         41.83363|    7.49125|Apollo14|    Antares (LM-8)|      15264| Kitty Hawk (CM-110)|       5758|          29|         149|           21022|              178|           0.483264|           0.00199|       9.62E-4|\n",
      "|          75.3991|   33.56547|Apollo15|    Falcon (LM-10)|      16430|   Endeavor (CM-112)|       5875|        1166|         117|           22305|             1283|           0.512759|           0.00338|      0.001733|\n",
      "|         92.46262|   17.06352|Apollo16|     Orion (LM-11)|      16445|     Casper (CM-113)|       5840|          15|         -35|           22285|              -20|           0.512299|          0.004149|      0.002126|\n",
      "|        109.44402|    16.9814|Apollo17|Challenger (LM-12)|      16456|    America (CM-114)|       5960|          11|         120|           22416|              131|            0.51531|          0.004882|      0.002516|\n",
      "+-----------------+-----------+--------+------------------+-----------+--------------------+-----------+------------+------------+----------------+-----------------+-------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample-to-weight ratio\n",
    "\n",
    "saturnVPayload = 43500\n",
    "\n",
    "missions = missions.withColumn(\"Crewed_area_Payload\", col(\"Total_weight(kg)\") / saturnVPayload) \\\n",
    "    .withColumn(\"Sample_Crewed_area\", col(\"Sample_weight(kg)\") / col(\"Total_weight(kg)\")) \\\n",
    "    .withColumn(\"Sample_Payload\", round(col(\"Sample_weight(kg)\") / saturnVPayload,6))\n",
    "\n",
    "# Round all numeric columns to 6 decimal places\n",
    "for c_name, c_type in missions.dtypes:\n",
    "    if c_type in ('long','double', 'float'):\n",
    "        missions = missions.withColumn(c_name, round(c_name, 6))\n",
    "\n",
    "missions.printSchema()\n",
    "missions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c66620d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o247.saveAsTable.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Update (managed) Missions table on train database\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m missions\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.Missions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect * From train.Missions;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDESCRIBE TABLE train.Missions;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\manso\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o247.saveAsTable.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1933)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\r\n"
     ]
    }
   ],
   "source": [
    "# Update (managed) Missions table on train database\n",
    "\n",
    "missions.write.mode(\"Overwrite\").saveAsTable(\"train.Missions\")\n",
    "\n",
    "spark.sql(\"Select * From train.Missions;\").show()\n",
    "spark.sql(\"DESCRIBE TABLE train.Missions;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1c07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the average of all those ratios across all the missions\n",
    "\n",
    "# Note: (...).collect()[0][0] was used to allow a single value in variable instead of dataframe\n",
    "crewedArea_payload_ratio = missions.select(round(_mean(\"Crewed_area_Payload\"),6)).collect()[0][0]\n",
    "sample_crewedArea_ratio = missions.select(round(_mean(\"Sample_Crewed_area\"),6)).collect()[0][0]\n",
    "sample_payload_ratio = missions.select(round(_mean(col(\"Sample_Payload\")),6)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab2bc3",
   "metadata": {},
   "source": [
    "# Exercise - Predict Artemis sample capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6cb5cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('artemis1', 26520, 26988) ('artemis1b', 26520, 37965) ('artemis2', 26520, 42955)\n",
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|\n",
      "+---------+----------------+-----------+\n",
      "| artemis1|           26520|      26988|\n",
      "|artemis1b|           26520|      37965|\n",
      "| artemis2|           26520|      42955|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Artemis mission DataFrame\n",
    "artemis_crewedArea = 26520\n",
    "\n",
    "missionData = [\"artemis1\",\"artemis1b\",\"artemis2\"]\n",
    "weightData = [artemis_crewedArea,artemis_crewedArea,artemis_crewedArea]\n",
    "payloadData = [26988, 37965, 42955]\n",
    "\n",
    "# joins / Zip the 3 configuration lists\n",
    "artemisAllData = zip(missionData,weightData,payloadData)\n",
    "\n",
    "# Convert zip object to a list\n",
    "artemisAllData = list(artemisAllData)\n",
    "\n",
    "#print(artemisAllData)\n",
    "print(*artemisAllData)\n",
    "\n",
    "# Schema configuration\n",
    "schemaArtemis = [\"Mission\", \"Total_weight(kg)\", \"Payload(kg)\"]\n",
    "\n",
    "artemis_mission = spark.createDataFrame(data = artemisAllData, schema = schemaArtemis)\n",
    "\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ec5cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      " |-- Sample_weight_from_total(kg): double (nullable = true)\n",
      " |-- Sample_weight_from_payload(kg): double (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "| artemis1|           26520|      26988|                    75.55548|                     38.781756|\n",
      "|artemis1b|           26520|      37965|                    75.55548|                     54.555705|\n",
      "| artemis2|           26520|      42955|                    75.55548|                     61.726335|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate the weight of samples based on the ratios we determined from the Apollo missions\n",
    "\n",
    "artemis_mission = artemis_mission.withColumn(\"Sample_weight_from_total(kg)\",round(col(\"Total_weight(kg)\") * sample_crewedArea_ratio,6))\n",
    "artemis_mission = artemis_mission.withColumn(\"Sample_weight_from_payload(kg)\",round(col(\"Payload(kg)\") * sample_payload_ratio,6))\n",
    "\n",
    "\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fe41cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "|summary| Mission|Total_weight(kg)|       Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|Estimated_sample_weight(kg)|\n",
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "|  count|       3|               3|                 3|                           3|                             3|                          3|\n",
      "|   mean|    NULL|         26520.0|35969.333333333336|                    75.55548|            51.687931999999996|          63.62170633333333|\n",
      "| stddev|    NULL|             0.0| 8168.432305732437|                         0.0|             11.73803722333751|          5.869018886548102|\n",
      "|    min|artemis1|           26520|             26988|                    75.55548|                     38.781756|                  57.168618|\n",
      "|    max|artemis2|           26520|             42955|                    75.55548|                     61.726335|                  68.640908|\n",
      "+-------+--------+----------------+------------------+----------------------------+------------------------------+---------------------------+\n",
      "\n",
      "root\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Total_weight(kg): long (nullable = true)\n",
      " |-- Payload(kg): long (nullable = true)\n",
      " |-- Sample_weight_from_total(kg): double (nullable = true)\n",
      " |-- Sample_weight_from_payload(kg): double (nullable = true)\n",
      " |-- Estimated_sample_weight(kg): double (nullable = true)\n",
      "\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "|  Mission|Total_weight(kg)|Payload(kg)|Sample_weight_from_total(kg)|Sample_weight_from_payload(kg)|Estimated_sample_weight(kg)|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "| artemis1|           26520|      26988|                    75.55548|                     38.781756|                  57.168618|\n",
      "|artemis1b|           26520|      37965|                    75.55548|                     54.555705|                  65.055593|\n",
      "| artemis2|           26520|      42955|                    75.55548|                     61.726335|                  68.640908|\n",
      "+---------+----------------+-----------+----------------------------+------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the average of the two predictions:\n",
    "artemis_mission = artemis_mission.withColumn(\"Estimated_sample_weight(kg)\", round((col(\"Sample_weight_from_payload(kg)\") + col(\"Sample_weight_from_total(kg)\"))/2,6))\n",
    "\n",
    "artemis_mission.describe().show()\n",
    "artemis_mission.printSchema()\n",
    "artemis_mission.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7730b",
   "metadata": {},
   "source": [
    "# Exercise - Prioritize Moon rock sample gathering based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ff2fbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Subtype: string (nullable = true)\n",
      " |-- Weight(g): double (nullable = true)\n",
      " |-- Pristine (%): double (nullable = true)\n",
      " |-- Weight(kg): double (nullable = true)\n",
      " |-- Remaining(kg): double (nullable = true)\n",
      "\n",
      "+-----+--------+------+--------+---------+------------+------------------+-------------+\n",
      "|   ID| Mission|  Type| Subtype|Weight(g)|Pristine (%)|        Weight(kg)|Remaining(kg)|\n",
      "+-----+--------+------+--------+---------+------------+------------------+-------------+\n",
      "|10001|Apollo11|  Soil|Unsieved|    125.8|       88.36|            0.1258|     0.111157|\n",
      "|10002|Apollo11|  Soil|Unsieved|   5629.0|       93.73|5.6290000000000004|     5.276062|\n",
      "|10003|Apollo11|Basalt|Ilmenite|    213.0|       65.56|             0.213|     0.139643|\n",
      "|10004|Apollo11|  Core|Unsieved|     44.8|       71.76|            0.0448|     0.032148|\n",
      "|10005|Apollo11|  Core|Unsieved|     53.4|       40.31|            0.0534|     0.021526|\n",
      "+-----+--------+------+--------+---------+------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine how much remains of each sample that was returned from the Apollo missions by multiplying the weight of the sample that was originally collected \n",
    "# by the percentage of remaining pristine sample.\n",
    "\n",
    "rock_samples = df\n",
    "\n",
    "rock_samples = rock_samples.withColumn(\"Remaining(kg)\", round(col(\"Weight(kg)\") * (col(\"Pristine (%)\") * .01), 6))\n",
    "\n",
    "rock_samples.printSchema()\n",
    "rock_samples.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2be4b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+-------+---------+------------------+------------------+-------------------+-------------------+\n",
      "|summary|               ID| Mission|   Type|  Subtype|         Weight(g)|      Pristine (%)|         Weight(kg)|      Remaining(kg)|\n",
      "+-------+-----------------+--------+-------+---------+------------------+------------------+-------------------+-------------------+\n",
      "|  count|             2229|    2229|   2229|     2226|              2229|              2229|               2229|               2229|\n",
      "|   mean|52058.43203230148|    NULL|   NULL|     NULL|168.25302377747855|  84.5127635711082|0.16825302377747908|0.13810338941229278|\n",
      "| stddev|26207.65147099608|    NULL|   NULL|     NULL| 637.2864579561242|22.057298716073344| 0.6372864579561255| 0.5259540237327816|\n",
      "|    min|            10001|Apollo11| Basalt|    1-2mm|               0.0|               0.0|                0.0|                0.0|\n",
      "|    max|            79537|Apollo17|Special|unstudied|           11729.0|             180.0| 11.729000000000001|          11.169527|\n",
      "+-------+-----------------+--------+-------+---------+------------------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame isn't useful for examining more than 2,000 samples. To get a better understanding of what the dataset contains, you can use the describe() function:\n",
    "\n",
    "rock_samples.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5f7fee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Subtype: string (nullable = true)\n",
      " |-- Weight(g): double (nullable = true)\n",
      " |-- Pristine (%): double (nullable = true)\n",
      " |-- Weight(kg): double (nullable = true)\n",
      " |-- Remaining(kg): double (nullable = true)\n",
      "\n",
      "+-----+--------+-------+--------+---------+------------+----------+-------------+\n",
      "|   ID| Mission|   Type| Subtype|Weight(g)|Pristine (%)|Weight(kg)|Remaining(kg)|\n",
      "+-----+--------+-------+--------+---------+------------+----------+-------------+\n",
      "|10017|Apollo11| Basalt|Ilmenite|    973.0|       43.71|     0.973|     0.425298|\n",
      "|10020|Apollo11| Basalt|Ilmenite|    425.0|       27.88|     0.425|      0.11849|\n",
      "|10021|Apollo11|Breccia|Regolith|    250.0|       30.21|      0.25|     0.075525|\n",
      "|10045|Apollo11| Basalt| Olivine|    185.0|       12.13|     0.185|     0.022441|\n",
      "|10057|Apollo11| Basalt|Ilmenite|    919.0|       35.15|     0.919|     0.323029|\n",
      "+-----+--------+-------+--------+---------+------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On average, each sample weighs about .16 kg and has about 84% of the original amount remaining. \n",
    "# We can use this knowledge to determine which samples are likely running low\n",
    "\n",
    "low_samples = rock_samples.where((col(\"Weight(kg)\") >= .16) & (col(\"Pristine (%)\") <= 50))\n",
    "\n",
    "low_samples.printSchema()\n",
    "low_samples.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71074b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   ID             27 non-null     int32  \n",
      " 1   Mission        27 non-null     object \n",
      " 2   Type           27 non-null     object \n",
      " 3   Subtype        27 non-null     object \n",
      " 4   Weight(g)      27 non-null     float64\n",
      " 5   Pristine (%)   27 non-null     float64\n",
      " 6   Weight(kg)     27 non-null     float64\n",
      " 7   Remaining(kg)  27 non-null     float64\n",
      "dtypes: float64(4), int32(1), object(3)\n",
      "memory usage: 1.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_samples.toPandas().info()\n",
    "\n",
    "type(low_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e2f097b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Type: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   Type|\n",
      "+-------+\n",
      "|   Soil|\n",
      "| Basalt|\n",
      "|Breccia|\n",
      "|   Core|\n",
      "|Special|\n",
      "|Crustal|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See how many unique types we have across the low_samples and rock_samples DataFrames.\n",
    "\n",
    "dfUniqueLow = low_samples.select(\"Type\").distinct()\n",
    "dfUniqueRockS = rock_samples.select(\"Type\").distinct()\n",
    "\n",
    "dfUniqueLowTypes = dfUniqueLow.union(dfUniqueRockS) \\\n",
    "            .distinct()\n",
    "\n",
    "dfUniqueLowTypes.printSchema()\n",
    "dfUniqueLowTypes.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7e47fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Type|count|\n",
      "+-------+-----+\n",
      "| Basalt|   14|\n",
      "|Breccia|    8|\n",
      "|   Core|    1|\n",
      "|   Soil|    4|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In our low_samples DataFrame, how many of each type are considered low?\n",
    "\n",
    "low_samples.groupby(\"Type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Type\") \\\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6c91760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   ID             22 non-null     int32  \n",
      " 1   Mission        22 non-null     object \n",
      " 2   Type           22 non-null     object \n",
      " 3   Subtype        22 non-null     object \n",
      " 4   Weight(g)      22 non-null     float64\n",
      " 5   Pristine (%)   22 non-null     float64\n",
      " 6   Weight(kg)     22 non-null     float64\n",
      " 7   Remaining(kg)  22 non-null     float64\n",
      "dtypes: float64(4), int32(1), object(3)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Focus on the Basalt and Breccia rock types for the samples that we need to have collected:\n",
    "\n",
    "# Test - In case all Types were to be considered\n",
    "# auxFilterTypes = list(dfUniqueLowTypes.select(\"Type\"))\n",
    "\n",
    "# Filter fot specific Types (list)\n",
    "auxFilterTypes = [\"Basalt\",\"Breccia\"]\n",
    "\n",
    "needed_samples = low_samples.where(col(\"Type\").isin(auxFilterTypes))\n",
    "\n",
    "needed_samples.toPandas().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35975194",
   "metadata": {},
   "source": [
    "# Exercise - Develop a recommendation of Moon rock samples to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a8f8b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|   Type|sum(Weight(kg))|\n",
      "+-------+---------------+\n",
      "| Basalt|        17.4234|\n",
      "|Breccia|        10.1185|\n",
      "+-------+---------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|   Type|   sum(Weight(kg))|\n",
      "+-------+------------------+\n",
      "| Basalt| 93.14076999999995|\n",
      "|Breccia|168.88074999999992|\n",
      "|   Core|19.935870000000005|\n",
      "|Crustal|           4.74469|\n",
      "|   Soil| 87.58980999999994|\n",
      "|Special|            0.7441|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the total weight from the needed_samples DataFrame to the rock_samples DataFrame\n",
    "\n",
    "needed_samples.groupby(\"Type\") \\\n",
    "    .sum(\"Weight(kg)\") \\\n",
    "    .orderBy(\"Type\") \\\n",
    "    .show()\n",
    "\n",
    "\n",
    "rock_samples.groupby(\"Type\") \\\n",
    "    .sum(\"Weight(kg)\") \\\n",
    "    .orderBy(\"Type\") \\\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c69265e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----------+---------+------------+--------------------+-------------+\n",
      "|   ID| Mission|   Type|    Subtype|Weight(g)|Pristine (%)|          Weight(kg)|Remaining(kg)|\n",
      "+-----+--------+-------+-----------+---------+------------+--------------------+-------------+\n",
      "|15361|Apollo15|Crustal|Cataclastic|      0.9|       66.56|9.000000000000001E-4|      5.99E-4|\n",
      "|15362|Apollo15|Crustal|Cataclastic|      4.2|       56.88|0.004200000000000001|     0.002389|\n",
      "|15363|Apollo15|Crustal|Cataclastic|      0.5|        71.0|              5.0E-4|      3.55E-4|\n",
      "|15415|Apollo15|Crustal|Anorthosite|    269.4|       67.07|              0.2694|     0.180687|\n",
      "|15437|Apollo15|Crustal|Anorthosite|      1.0|        80.0|               0.001|       8.0E-4|\n",
      "|60057|Apollo16|Crustal|Anorthosite|      3.1|       100.0|0.003100000000000...|       0.0031|\n",
      "|60059|Apollo16|Crustal|Anorthosite|     1.05|       100.0|0.001050000000000...|      0.00105|\n",
      "|60619|Apollo16|Crustal|Anorthosite|     28.0|       90.75|               0.028|      0.02541|\n",
      "|61017|Apollo16|Crustal|Anorthosite|     2.62|       100.0|0.002620000000000...|      0.00262|\n",
      "|61226|Apollo16|Crustal|Anorthosite|     1.53|       100.0|0.001530000000000...|      0.00153|\n",
      "+-----+--------+-------+-----------+---------+------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Mission: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Subtype: string (nullable = true)\n",
      " |-- Weight(g): double (nullable = true)\n",
      " |-- Pristine (%): double (nullable = true)\n",
      " |-- Weight(kg): double (nullable = true)\n",
      " |-- Remaining(kg): double (nullable = true)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   ID             68 non-null     int32  \n",
      " 1   Mission        68 non-null     object \n",
      " 2   Type           68 non-null     object \n",
      " 3   Subtype        68 non-null     object \n",
      " 4   Weight(g)      68 non-null     float64\n",
      " 5   Pristine (%)   68 non-null     float64\n",
      " 6   Weight(kg)     68 non-null     float64\n",
      " 7   Remaining(kg)  68 non-null     float64\n",
      "dtypes: float64(4), int32(1), object(3)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Add Crustal rocks to the set of needed samples\n",
    "\n",
    "crustalSample = rock_samples.where(col(\"Type\") == 'Crustal')\n",
    "\n",
    "crustalSample.show(10)\n",
    "\n",
    "needed_samples = needed_samples.union(crustalSample)\n",
    "\n",
    "needed_samples.printSchema()\n",
    "needed_samples.toPandas().info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "465910f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Type|\n",
      "+-------+\n",
      "| Basalt|\n",
      "|Breccia|\n",
      "|Crustal|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need a column for each type of rock that we want more samples of:\n",
    "\n",
    "needed_samples_overview = needed_samples.select(\"Type\").distinct()\n",
    "\n",
    "needed_samples_overview.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "83b70e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Total_weight(kg): double (nullable = true)\n",
      "\n",
      "+-------+----------------+\n",
      "|   Type|Total_weight(kg)|\n",
      "+-------+----------------+\n",
      "| Basalt|         17.4234|\n",
      "|Breccia|         10.1185|\n",
      "|Crustal|         4.74469|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, we want the total weight of each type of rock that was originally collected:\n",
    "\n",
    "needed_sample_weights = needed_samples.groupby(\"Type\") \\\n",
    "    .sum(\"Weight(kg)\") \\\n",
    "    .orderBy(\"Type\") \n",
    "\n",
    "\n",
    "needed_samples_overview = needed_samples_overview.join(needed_sample_weights,needed_samples_overview[\"Type\"] == needed_sample_weights[\"Type\"]) \\\n",
    "        .withColumnRenamed(\"sum(Weight(kg))\", \"Total_weight(kg)\") \\\n",
    "        .drop(needed_samples_overview[\"Type\"]) \n",
    "        \n",
    "        \n",
    "needed_samples_overview.printSchema()\n",
    "needed_samples_overview.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fde1da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Total_weight(kg): double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Average_weight(kg): double (nullable = true)\n",
      "\n",
      "+----------------+-------+-------------------+\n",
      "|Total_weight(kg)|   Type| Average_weight(kg)|\n",
      "+----------------+-------+-------------------+\n",
      "|         17.4234| Basalt| 1.2445285714285714|\n",
      "|         10.1185|Breccia|          1.2648125|\n",
      "|         4.74469|Crustal|0.10314543478260871|\n",
      "+----------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When astronauts are up on the Moon, one way they can identify rocks is by their size. \n",
    "# If we can give them an estimated size of each type of rock that might make their collection process easier.\n",
    "\n",
    "needed_sample_ave_weights = needed_samples.groupby(\"Type\") \\\n",
    "    .mean(\"Weight(kg)\") \\\n",
    "    .withColumnRenamed(\"avg(Weight(kg))\", \"Average_weight(kg)\") \\\n",
    "    .orderBy(\"Type\") \n",
    "\n",
    "needed_samples_overview = needed_samples_overview.join(needed_sample_ave_weights,needed_samples_overview[\"Type\"] == needed_sample_ave_weights[\"Type\"]) \\\n",
    "        .withColumnRenamed(\"avg(Weight(kg))\", \"Average_weight(kg)\") \\\n",
    "        .drop(needed_samples_overview[\"Type\"])\n",
    "\n",
    "needed_samples_overview.printSchema()\n",
    "needed_samples_overview.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Total_weight(kg): double (nullable = true)\n",
      " |-- Average_weight(kg): double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Number_of_samples: long (nullable = false)\n",
      " |-- Percentage_of_rocks: double (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-------+-----------------+-------------------+\n",
      "|Total_weight(kg)| Average_weight(kg)|   Type|Number_of_samples|Percentage_of_rocks|\n",
      "+----------------+-------------------+-------+-----------------+-------------------+\n",
      "|         17.4234| 1.2445285714285714| Basalt|              351|            0.25885|\n",
      "|         10.1185|          1.2648125|Breccia|              959|           0.707227|\n",
      "|         4.74469|0.10314543478260871|Crustal|               46|           0.033923|\n",
      "+----------------+-------------------+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the three types we're looking for, we should grab the total number we have of each type and get the remaining percentage of each type of rock.\n",
    "\n",
    "total_rock_count = rock_samples.groupby(\"Type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Type\") \\\n",
    "    .withColumnRenamed(\"count\", \"Number_of_samples\")\n",
    "\n",
    "needed_samples_overview = needed_samples_overview.join(total_rock_count,needed_samples_overview[\"Type\"] == total_rock_count[\"Type\"]) \\\n",
    "        .drop(needed_samples_overview[\"Type\"]) \n",
    "\n",
    "total_rocks = needed_samples_overview.select(_sum(\"Number_of_samples\")).collect()[0][0]\n",
    "\n",
    "needed_samples_overview = needed_samples_overview.withColumn(\"Percentage_of_rocks\",round(col(\"Number_of_samples\") / total_rocks,6))\n",
    "\n",
    "needed_samples_overview.printSchema()\n",
    "needed_samples_overview.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4e57d01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.62170633333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the average weight of samples we estimated in the preceding unit.\n",
    "\n",
    "artemis_ave_weight = artemis_mission.select(_mean(\"Estimated_sample_weight(kg)\")).collect()[0][0]\n",
    "\n",
    "display(artemis_ave_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8020dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Total_weight(kg)     3 non-null      float64\n",
      " 1   Average_weight(kg)   3 non-null      float64\n",
      " 2   Type                 3 non-null      object \n",
      " 3   Number_of_samples    3 non-null      int64  \n",
      " 4   Percentage_of_rocks  3 non-null      float64\n",
      " 5   Weight_to_collect    3 non-null      float64\n",
      " 6   Rocks_to_collect     3 non-null      float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 300.0+ bytes\n",
      "+----------------+-------------------+-------+-----------------+-------------------+-----------------+----------------+\n",
      "|Total_weight(kg)| Average_weight(kg)|   Type|Number_of_samples|Percentage_of_rocks|Weight_to_collect|Rocks_to_collect|\n",
      "+----------------+-------------------+-------+-----------------+-------------------+-----------------+----------------+\n",
      "|         17.4234| 1.2445285714285714| Basalt|              351|            0.25885|        16.468479|       13.232705|\n",
      "|         10.1185|          1.2648125|Breccia|              959|           0.707227|        44.994989|       35.574434|\n",
      "|         4.74469|0.10314543478260871|Crustal|               46|           0.033923|         2.158239|       20.924232|\n",
      "+----------------+-------------------+-------+-----------------+-------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use this number to determine how many of each rock we want the astronauts to aim to collect:\n",
    "\n",
    "needed_samples_overview.toPandas().info()\n",
    "\n",
    "needed_samples_overview = needed_samples_overview.withColumn(\"Weight_to_collect\",round(col(\"Percentage_of_rocks\") * artemis_ave_weight,6))\n",
    "needed_samples_overview = needed_samples_overview.withColumn(\"Rocks_to_collect\",round(col(\"Weight_to_collect\") / col(\"Average_weight(kg)\"),6))\n",
    "\n",
    "#needed_samples_overview.printSchema()\n",
    "needed_samples_overview.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
